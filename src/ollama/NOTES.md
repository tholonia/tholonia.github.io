### /home/jw/.conda/envs/llama/bin/ollama serve
- 21263334
- http://localhost:11434/api/version  = {"version":"0.1.17"}
- GPU: No

### /bin/ollama
- 413612280
- http://localhost:11434/api/version  = {"version":"0.1.16"}
- GPU: Yes

### /usr/local/bin/ollama  
- 413696000
- http://localhost:11434/api/version  not working


Examples

- https://github.com/jmorganca/ollama/blob/main/docs/tutorials/langchainpy.md

